{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 5 Policy-Based Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Policy Gradient for MountainCar Gym (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import adam\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# env = gym.make('CartPole-v1')\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "env = env.unwrapped\n",
    "env.seed(1)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "plt.ion()\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.02\n",
    "gamma = 0.995\n",
    "episodes = 1000\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "action_space = env.action_space.n\n",
    "state_space = env.observation_space.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(state_space, 20)\n",
    "        #self.fc2 = nn.Linear(128,64)\n",
    "        self.fc3 = nn.Linear(20, action_space)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = adam.Adam(policy.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selct_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    c = Categorical(probs)\n",
    "    action = c.sample()\n",
    "\n",
    "    policy.saved_log_probs.append(c.log_prob(action))\n",
    "    action = action.item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    rewards = []\n",
    "\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + policy.gamma * R\n",
    "        rewards.insert(0, R)\n",
    "\n",
    "    # Formalize reward\n",
    "    rewards = torch.tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean())/(rewards.std() + eps)\n",
    "\n",
    "    # get loss\n",
    "    for reward, log_prob in zip(rewards, policy.saved_log_probs):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(steps):\n",
    "    ax = plt.subplot(111)\n",
    "    ax.cla()\n",
    "    ax.set_title('Training')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Run Time')\n",
    "    ax.plot(steps)\n",
    "    RunTime = len(steps)\n",
    "    path =  './PG_MountainCar-v0/'+'RunTime'+str(RunTime)+'.jpg'\n",
    "    if len(steps) % 100 == 0:\n",
    "        plt.savefig(path)\n",
    "    plt.pause(0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    running_reward = 0\n",
    "    steps = []\n",
    "    for episode in count(60000):\n",
    "        state = env.reset()\n",
    "\n",
    "        for t in range(10000):\n",
    "            action = selct_action(state)\n",
    "            state, reward ,done, info = env.step(action)\n",
    "            env.render()\n",
    "            policy.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                print(\"Episode {}, live time = {}\".format(episode, t))\n",
    "                steps.append(t)\n",
    "                plot(steps)\n",
    "                break\n",
    "        if episode % 50 == 0:\n",
    "            torch.save(policy, 'policyNet.pkl')\n",
    "\n",
    "        running_reward = running_reward * policy.gamma - t*0.01\n",
    "        finish_episode()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Policy Gradient for MountainCar Gym (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "\n",
    "import gym\n",
    "#env = gym.make(\"CartPole-v0\")\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# Constants defining our neural network\n",
    "hidden_layer_neurons = 8\n",
    "gamma = .99\n",
    "dimen = len(env.reset())\n",
    "print_every = 100\n",
    "batch_size = 50\n",
    "num_episodes = 10000\n",
    "render = False\n",
    "lr = 1e-2\n",
    "goal = 190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_model(env, hidden_layer_neurons, lr):\n",
    "    dimen = env.reset().shape\n",
    "    num_actions = env.action_space.n\n",
    "    inp = layers.Input(shape=dimen,name=\"input_x\")\n",
    "    adv = layers.Input(shape=[1], name=\"advantages\")\n",
    "    x = layers.Dense(hidden_layer_neurons, \n",
    "                     activation=\"relu\", \n",
    "                     use_bias=False,\n",
    "                     kernel_initializer=glorot_uniform(seed=42),\n",
    "                     name=\"dense_1\")(inp)\n",
    "    out = layers.Dense(num_actions, \n",
    "                       activation=\"softmax\", \n",
    "                       kernel_initializer=glorot_uniform(seed=42),\n",
    "                       use_bias=False,\n",
    "                       name=\"out\")(x)\n",
    "\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        # actual: 0 predict: 0 -> log(0 * (0 - 0) + (1 - 0) * (0 + 0)) = -inf\n",
    "        # actual: 1 predict: 1 -> log(1 * (1 - 1) + (1 - 1) * (1 + 1)) = -inf\n",
    "        # actual: 1 predict: 0 -> log(1 * (1 - 0) + (1 - 1) * (1 + 0)) = 0\n",
    "        # actual: 0 predict: 1 -> log(0 * (0 - 1) + (1 - 0) * (0 + 1)) = 0\n",
    "        log_lik = K.log(y_true * (y_true - y_pred) + (1 - y_true) * (y_true + y_pred))\n",
    "        return K.mean(log_lik * adv, keepdims=True)\n",
    "        \n",
    "    model_train = Model(inputs=[inp, adv], outputs=out)\n",
    "    model_train.compile(loss=custom_loss, optimizer=Adam(lr))\n",
    "    model_predict = Model(inputs=[inp], outputs=out)\n",
    "    return model_train, model_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
    "    e.g. f([1, 1, 1], 0.99) -> [2.9701, 1.99, 1]\n",
    "    \"\"\"\n",
    "    prior = 0\n",
    "    out = []\n",
    "    for val in r:\n",
    "        new_val = val + prior * gamma\n",
    "        out.append(new_val)\n",
    "        prior = new_val\n",
    "    return np.array(out[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See our trained bot in action\n",
    "def score_model(model, num_tests, render=False):\n",
    "    scores = []    \n",
    "    for num_test in range(num_tests):\n",
    "        observation = env.reset()\n",
    "        reward_sum = 0\n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            state = np.reshape(observation, [1, dimen])\n",
    "            predict = model.predict([state])[0]\n",
    "            action = np.argmax(predict)\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            reward_sum += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores.append(reward_sum)\n",
    "    env.close()\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train, model_predict = get_policy_model(env, hidden_layer_neurons, lr)\n",
    "model_predict.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_sum = 0\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Placeholders for our observations, outputs and rewards\n",
    "states = np.empty(0).reshape(0,dimen)\n",
    "actions = np.empty(0).reshape(0,1)\n",
    "rewards = np.empty(0).reshape(0,1)\n",
    "discounted_rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "# Setting up our environment\n",
    "observation = env.reset()\n",
    "\n",
    "num_episode = 0\n",
    "\n",
    "losses = []\n",
    "\n",
    "while num_episode < num_episodes:\n",
    "    # Append the observations to our batch\n",
    "    state = np.reshape(observation, [1, dimen])\n",
    "    \n",
    "    predict = model_predict.predict([state])[0]\n",
    "    action = np.random.choice(range(num_actions),p=predict)\n",
    "    \n",
    "    # Append the observations and outputs for learning\n",
    "    states = np.vstack([states, state])\n",
    "    actions = np.vstack([actions, action])\n",
    "    \n",
    "    # Determine the oucome of our action\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    rewards = np.vstack([rewards, reward])\n",
    "    \n",
    "    if done:\n",
    "        # Determine standardized rewards\n",
    "        discounted_rewards_episode = discount_rewards(rewards, gamma)       \n",
    "        discounted_rewards = np.vstack([discounted_rewards, discounted_rewards_episode])\n",
    "        \n",
    "        rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "        if (num_episode + 1) % batch_size == 0:\n",
    "            discounted_rewards -= discounted_rewards.mean()\n",
    "            discounted_rewards /= discounted_rewards.std()\n",
    "            discounted_rewards = discounted_rewards.squeeze()\n",
    "            actions = actions.squeeze().astype(int)\n",
    "           \n",
    "            actions_train = np.zeros([len(actions), num_actions])\n",
    "            actions_train[np.arange(len(actions)), actions] = 1\n",
    "            \n",
    "            loss = model_train.train_on_batch([states, discounted_rewards], actions_train)\n",
    "            losses.append(loss)\n",
    "\n",
    "            # Clear out game variables\n",
    "            states = np.empty(0).reshape(0,dimen)\n",
    "            actions = np.empty(0).reshape(0,1)\n",
    "            discounted_rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "\n",
    "        # Print periodically\n",
    "        if (num_episode + 1) % print_every == 0:\n",
    "            # Print status\n",
    "            score = score_model(model_predict,10)\n",
    "            print(\"Average reward for training episode {}: {:0.2f} Test Score: {:0.2f} Loss: {:0.6f} \".format(\n",
    "                (num_episode + 1), reward_sum/print_every, \n",
    "                score,\n",
    "                np.mean(losses[-print_every:])))\n",
    "            \n",
    "            if score >= goal:\n",
    "                print(\"Solved in {} episodes!\".format(num_episode))\n",
    "                break\n",
    "            reward_sum = 0\n",
    "                \n",
    "        num_episode += 1\n",
    "        observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
