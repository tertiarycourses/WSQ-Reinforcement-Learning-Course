{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 5 Overview of Advanced RL Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C and A3C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C on Cartpole Demo - Single Envirnoment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "model = A2C('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _state = model.predict(obs,deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(f'Episode:{episode} Score:{score}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3C on Cartpole Demo - Multiple Envirnoments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Parallel environments\n",
    "env = make_vec_env(\"CartPole-v0\", n_envs=4)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: A2C on Taxi Environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "model = A2C(______________________)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "episodes = 5\n",
    "total_score = 0\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _state = _________________\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(f'Episode:{episode} Score:{score}')\n",
    "    total_score += score\n",
    "    \n",
    "env.close()\n",
    "ave_score = total_score/episodes\n",
    "print(f'Average Score:{ave_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: A2C on Taxi Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 415      |\n",
      "|    ep_rew_mean        | -414     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1042     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | -0.34    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -2.72    |\n",
      "|    value_loss         | 7.58     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 264      |\n",
      "|    ep_rew_mean        | -263     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1058     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.0997   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.527    |\n",
      "|    value_loss         | 1.77     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 257      |\n",
      "|    ep_rew_mean        | -256     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1071     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.881   |\n",
      "|    explained_variance | -0.0526  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -2.27    |\n",
      "|    value_loss         | 5.87     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 233      |\n",
      "|    ep_rew_mean        | -232     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1077     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.873   |\n",
      "|    explained_variance | -1.12    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 1.51     |\n",
      "|    value_loss         | 6.76     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 197      |\n",
      "|    ep_rew_mean        | -196     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1080     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.791   |\n",
      "|    explained_variance | -5.54    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -8.25    |\n",
      "|    value_loss         | 38.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 191      |\n",
      "|    ep_rew_mean        | -190     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1082     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.681   |\n",
      "|    explained_variance | 0.0693   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -1.12    |\n",
      "|    value_loss         | 4.57     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 183      |\n",
      "|    ep_rew_mean        | -182     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1081     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | -0.00214 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -2.55    |\n",
      "|    value_loss         | 4.28     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 180      |\n",
      "|    ep_rew_mean        | -179     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1078     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.643   |\n",
      "|    explained_variance | 0.000547 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -1.14    |\n",
      "|    value_loss         | 3.68     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 170      |\n",
      "|    ep_rew_mean        | -169     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1073     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.763   |\n",
      "|    explained_variance | -0.00525 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -1.1     |\n",
      "|    value_loss         | 3.15     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 164      |\n",
      "|    ep_rew_mean        | -163     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1070     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.697   |\n",
      "|    explained_variance | 0.0136   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.58    |\n",
      "|    value_loss         | 2.64     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 159      |\n",
      "|    ep_rew_mean        | -158     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1064     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.544   |\n",
      "|    explained_variance | -0.0193  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.362   |\n",
      "|    value_loss         | 2.3      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 152      |\n",
      "|    ep_rew_mean        | -151     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1060     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.696   |\n",
      "|    explained_variance | -0.0253  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.87    |\n",
      "|    value_loss         | 1.89     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 146      |\n",
      "|    ep_rew_mean        | -145     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1055     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.682   |\n",
      "|    explained_variance | 0.0505   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.269   |\n",
      "|    value_loss         | 1.53     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 145       |\n",
      "|    ep_rew_mean        | -144      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1055      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.753    |\n",
      "|    explained_variance | -0.000636 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -1.42     |\n",
      "|    value_loss         | 1.19      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 141      |\n",
      "|    ep_rew_mean        | -140     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1053     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.658   |\n",
      "|    explained_variance | 0.000149 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.728   |\n",
      "|    value_loss         | 0.921    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 139      |\n",
      "|    ep_rew_mean        | -138     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1054     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.334   |\n",
      "|    explained_variance | 0.00696  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.743   |\n",
      "|    value_loss         | 0.704    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 134       |\n",
      "|    ep_rew_mean        | -133      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1051      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.662    |\n",
      "|    explained_variance | -1.42e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -0.276    |\n",
      "|    value_loss         | 0.478     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 134      |\n",
      "|    ep_rew_mean        | -133     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1049     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.918   |\n",
      "|    explained_variance | 0.000394 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 1.05     |\n",
      "|    value_loss         | 1.38e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 132      |\n",
      "|    ep_rew_mean        | -131     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1047     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.792   |\n",
      "|    explained_variance | 1.72e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 40       |\n",
      "|    value_loss         | 5.88e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 130      |\n",
      "|    ep_rew_mean        | -129     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1047     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.463   |\n",
      "|    explained_variance | 0.0167   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.0325  |\n",
      "|    value_loss         | 0.0785   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-22 00:36:09.758 python[69525:5964678] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f97a9b9d120>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-22 00:36:09.759 python[69525:5964678] Warning: Expected min height of view: (<NSButton: 0x7f97b5630980>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-22 00:36:09.761 python[69525:5964678] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f97b5633050>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-22 00:36:09.763 python[69525:5964678] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f97b563ce10>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-78.0\n",
      "Episode:2 Score:-81.0\n",
      "Episode:3 Score:-82.0\n",
      "Episode:4 Score:-106.0\n",
      "Episode:5 Score:-75.0\n",
      "Average Score:-84.4\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "env = gym.make('Acrobot-v1')\n",
    "\n",
    "model = A2C('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "episodes = 5\n",
    "total_score = 0\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(f'Episode:{episode} Score:{score}')\n",
    "    total_score += score\n",
    "    \n",
    "env.close()\n",
    "avg_score = total_score/episodes\n",
    "print(f'Average Score:{avg_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "model.save(\"ppo_cartpole\")\n",
    "del model # remove to demonstrate saving and loading\n",
    "model = PPO.load(\"ppo_cartpole\")\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _state = model.predict(obs,deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(f'Episode:{episode} Score:{score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "model = PPO.load(\"ppo_cartpole\")\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO.load(\"ppo_cartpole\")\n",
    "\n",
    "episode = 1\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _state = model.predict(obs,deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(f'Episode:{episode} Score:{score}')\n",
    "    episode +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: PPO on Acrobot Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = gym.make('Acrobot-v1')\n",
    "\n",
    "model = _____________________________\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "model.save(_________________)\n",
    "del model # remove to demonstrate saving and loading\n",
    "model = PPO.load(_______________)\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _state = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(f'Episode:{episode} Score:{score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "model = PPO.load(__________________)\n",
    "evaluate_policy(___________________________)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: PPO on Arobot Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = gym.make('Acrobot-v1')\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "model.save(\"ppo_taxi\")\n",
    "del model # remove to demonstrate saving and loading\n",
    "model = PPO.load(\"ppo_taxi\")\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _state = model.predict(obs,deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(f'Episode:{episode} Score:{score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "model = PPO.load(\"ppo_taxi\")\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
