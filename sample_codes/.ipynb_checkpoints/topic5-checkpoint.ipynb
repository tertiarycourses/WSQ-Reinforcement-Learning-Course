{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 6 Overview of Advanced RL Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 22.4     |\n",
      "|    ep_rew_mean        | 22.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1143     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.649   |\n",
      "|    explained_variance | -0.58    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -1.16    |\n",
      "|    value_loss         | 34.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 20.3     |\n",
      "|    ep_rew_mean        | 20.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1144     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.664   |\n",
      "|    explained_variance | -0.05    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.44     |\n",
      "|    value_loss         | 8.3      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 19.7     |\n",
      "|    ep_rew_mean        | 19.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1149     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.581   |\n",
      "|    explained_variance | 0.0334   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 1.42     |\n",
      "|    value_loss         | 6.93     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 20.5     |\n",
      "|    ep_rew_mean        | 20.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1152     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.583   |\n",
      "|    explained_variance | 0.0478   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 1.19     |\n",
      "|    value_loss         | 7.84     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 21.6     |\n",
      "|    ep_rew_mean        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1176     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.57    |\n",
      "|    explained_variance | 0.0802   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.859    |\n",
      "|    value_loss         | 6.39     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 24.4     |\n",
      "|    ep_rew_mean        | 24.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1193     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.497   |\n",
      "|    explained_variance | -0.00841 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 1.19     |\n",
      "|    value_loss         | 5.56     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 27.8     |\n",
      "|    ep_rew_mean        | 27.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1206     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.617   |\n",
      "|    explained_variance | 0.00769  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.938    |\n",
      "|    value_loss         | 5.13     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 31       |\n",
      "|    ep_rew_mean        | 31       |\n",
      "| time/                 |          |\n",
      "|    fps                | 1212     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.633   |\n",
      "|    explained_variance | 0.00231  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.863    |\n",
      "|    value_loss         | 4.61     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 35.8      |\n",
      "|    ep_rew_mean        | 35.8      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1202      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.541    |\n",
      "|    explained_variance | -0.000524 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -11.9     |\n",
      "|    value_loss         | 797       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 38.2     |\n",
      "|    ep_rew_mean        | 38.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1194     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.616   |\n",
      "|    explained_variance | 0.00725  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.833    |\n",
      "|    value_loss         | 3.42     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 45.1     |\n",
      "|    ep_rew_mean        | 45.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1196     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.574   |\n",
      "|    explained_variance | 0.00183  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.547    |\n",
      "|    value_loss         | 2.98     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 49.9     |\n",
      "|    ep_rew_mean        | 49.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1199     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.602   |\n",
      "|    explained_variance | 0.000713 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.71     |\n",
      "|    value_loss         | 2.51     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 54.4     |\n",
      "|    ep_rew_mean        | 54.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1206     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.392   |\n",
      "|    explained_variance | 0.0026   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 1.16     |\n",
      "|    value_loss         | 2.08     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 57.8      |\n",
      "|    ep_rew_mean        | 57.8      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1202      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.459    |\n",
      "|    explained_variance | -7.07e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 0.62      |\n",
      "|    value_loss         | 1.69      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 62.6      |\n",
      "|    ep_rew_mean        | 62.6      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1198      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.498    |\n",
      "|    explained_variance | -0.000585 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 0.35      |\n",
      "|    value_loss         | 1.36      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 66.4      |\n",
      "|    ep_rew_mean        | 66.4      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1198      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.546    |\n",
      "|    explained_variance | -4.48e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 0.463     |\n",
      "|    value_loss         | 1.05      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 70.8     |\n",
      "|    ep_rew_mean        | 70.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1202     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.546   |\n",
      "|    explained_variance | 5.82e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.306    |\n",
      "|    value_loss         | 0.78     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 75.9      |\n",
      "|    ep_rew_mean        | 75.9      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1199      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.604    |\n",
      "|    explained_variance | -6.06e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 0.244     |\n",
      "|    value_loss         | 0.558     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 80.6      |\n",
      "|    ep_rew_mean        | 80.6      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1202      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.567    |\n",
      "|    explained_variance | -0.000863 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 0.213     |\n",
      "|    value_loss         | 0.364     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 82.8      |\n",
      "|    ep_rew_mean        | 82.8      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1203      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.605    |\n",
      "|    explained_variance | -2.22e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 0.151     |\n",
      "|    value_loss         | 0.209     |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 00:44:23.664 python[48985:4991242] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa34f925740>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 00:44:23.665 python[48985:4991242] Warning: Expected min height of view: (<NSButton: 0x7fa351fb6710>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 00:44:23.667 python[48985:4991242] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa351f9f2b0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 00:44:23.669 python[48985:4991242] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa351f92400>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "model = A2C('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "      obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Parallel environments\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"a2c_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = A2C.load(\"a2c_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | 22       |\n",
      "| time/              |          |\n",
      "|    fps             | 8283     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 35          |\n",
      "|    ep_rew_mean          | 35          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3508        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013800575 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | -0.00164    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.75        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 19.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 72.6        |\n",
      "|    ep_rew_mean          | 72.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2975        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014042374 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.647      |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    value_loss           | 25.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 107         |\n",
      "|    ep_rew_mean          | 107         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2776        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011534424 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.605      |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.5        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 44.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 01:15:01.672 python[48985:4991242] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa354294fc0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 01:15:01.672 python[48985:4991242] Warning: Expected min height of view: (<NSButton: 0x7fa351fed320>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 01:15:01.675 python[48985:4991242] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa351fefd10>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 01:15:01.676 python[48985:4991242] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa351f84a70>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 01:15:01.855 python[48985:4991242] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa3542b6e20>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 01:15:01.856 python[48985:4991242] Warning: Expected min height of view: (<NSButton: 0x7fa3542d4ad0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 01:15:01.858 python[48985:4991242] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa3542d65c0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 01:15:01.860 python[48985:4991242] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa3542dc220>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 01:15:01.963 python[48985:4991242] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa3539c39c0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 01:15:01.963 python[48985:4991242] Warning: Expected min height of view: (<NSButton: 0x7fa354751230>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 01:15:01.966 python[48985:4991242] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa354753c10>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 01:15:01.968 python[48985:4991242] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa354759840>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 01:15:02.073 python[48985:4991242] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa3539fd0e0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 01:15:02.073 python[48985:4991242] Warning: Expected min height of view: (<NSButton: 0x7fa35471a030>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 01:15:02.076 python[48985:4991242] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa354730a30>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2021-11-20 01:15:02.077 python[48985:4991242] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa354736690>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Parallel environments\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"ppo_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = PPO.load(\"ppo_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = \"CartPole-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 50\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-push cart to left, 1-push cart to the right\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done: \n",
    "        print('info', info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=190, verbose=1)\n",
    "eval_callback = EvalCallback(env, \n",
    "                             callback_on_new_best=stop_callback, \n",
    "                             eval_freq=10000, \n",
    "                             best_model_save_path=save_path, \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('Training', 'Saved Models', 'best_model')\n",
    "model = PPO.load(model_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: DQN on CartPole Gym (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "EPSILON = 0.9\n",
    "GAMMA = 0.9\n",
    "LR = 0.01\n",
    "MEMORY_CAPACITY = 2000\n",
    "Q_NETWORK_ITERATION = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EPISODES = 400\n",
    "# env = gym.make('MountainCar-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "env = env.unwrapped\n",
    "NUM_STATES = env.observation_space.shape[0] # 2\n",
    "NUM_ACTIONS = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(NUM_STATES, 30)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)\n",
    "        self.fc2 = nn.Linear(30, NUM_ACTIONS)\n",
    "        self.fc2.weight.data.normal_(0, 0.1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dqn():\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, NUM_STATES *2 +2))\n",
    "        # state, action ,reward and next state\n",
    "        self.memory_counter = 0\n",
    "        self.learn_counter = 0\n",
    "        self.optimizer = optim.Adam(self.eval_net.parameters(), LR)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "\n",
    "    def store_trans(self, state, action, reward, next_state):\n",
    "        if self.memory_counter % 500 ==0:\n",
    "            print(\"The experience pool collects {} time experience\".format(self.memory_counter))\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        trans = np.hstack((state, [action], [reward], next_state))\n",
    "        self.memory[index,] = trans\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # notation that the function return the action's index nor the real action\n",
    "        # EPSILON\n",
    "        state = torch.unsqueeze(torch.FloatTensor(state) ,0)\n",
    "        if np.random.randn() <= EPSILON:\n",
    "            action_value = self.eval_net.forward(state)\n",
    "            action = torch.max(action_value, 1)[1].data.numpy() # get action whose q is max\n",
    "            action = action[0] #get the action index\n",
    "        else:\n",
    "            action = np.random.randint(0,NUM_ACTIONS)\n",
    "        return action\n",
    "\n",
    "    def plot(self, ax, x):\n",
    "        ax.cla()\n",
    "        ax.set_xlabel(\"episode\")\n",
    "        ax.set_ylabel(\"total reward\")\n",
    "        ax.plot(x, 'b-')\n",
    "        plt.pause(0.000000000000001)\n",
    "\n",
    "    def learn(self):\n",
    "        # learn 100 times then the target network update\n",
    "        if self.learn_counter % Q_NETWORK_ITERATION ==0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_counter+=1\n",
    "\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "        batch_state = torch.FloatTensor(batch_memory[:, :NUM_STATES])\n",
    "        #note that the action must be a int\n",
    "        batch_action = torch.LongTensor(batch_memory[:, NUM_STATES:NUM_STATES+1].astype(int))\n",
    "        batch_reward = torch.FloatTensor(batch_memory[:, NUM_STATES+1: NUM_STATES+2])\n",
    "        batch_next_state = torch.FloatTensor(batch_memory[:, -NUM_STATES:])\n",
    "\n",
    "        q_eval = self.eval_net(batch_state).gather(1, batch_action)\n",
    "        q_next = self.target_net(batch_next_state).detach()\n",
    "        q_target = batch_reward + GAMMA*q_next.max(1)[0].view(BATCH_SIZE, 1)\n",
    "\n",
    "        loss = self.loss(q_eval, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    net = Dqn()\n",
    "    print(\"The DQN is collecting experience...\")\n",
    "    step_counter_list = []\n",
    "    for episode in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        step_counter = 0\n",
    "        while True:\n",
    "            step_counter +=1\n",
    "            env.render()\n",
    "            action = net.choose_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            reward = reward * 100 if reward >0 else reward * 5\n",
    "            net.store_trans(state, action, reward, next_state)\n",
    "\n",
    "            if net.memory_counter >= MEMORY_CAPACITY:\n",
    "                net.learn()\n",
    "                if done:\n",
    "                    print(\"episode {}, the reward is {}\".format(episode, round(reward, 3)))\n",
    "            if done:\n",
    "                step_counter_list.append(step_counter)\n",
    "                net.plot(net.ax, step_counter_list)\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: DQN on CartPole Gym (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def OurModel(input_shape, action_space):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "    X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='CartPole DQN model')\n",
    "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        \n",
    "        # by default, CartPole-v1 has max episode steps = 500\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES = 1000\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        \n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "\n",
    "        # create main model\n",
    "        self.model = OurModel(input_shape=(self.state_size,), action_space = self.action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > self.train_start:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        # Randomly sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "\n",
    "        state = np.zeros((self.batch_size, self.state_size))\n",
    "        next_state = np.zeros((self.batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        # do batch prediction to save speed\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model.predict(next_state)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # Standard - DQN\n",
    "                # DQN chooses the max Q value among next actions\n",
    "                # selection and evaluation of action is on the target Q Network\n",
    "                # Q_max = max_a' Q_target(s', a')\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "            \n",
    "    def run(self):\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                if not done or i == self.env._max_episode_steps-1:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -100\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                if done:                   \n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, self.EPISODES, i, self.epsilon))\n",
    "                    if i == 500:\n",
    "                        print(\"Saving trained model as cartpole-dqn.h5\")\n",
    "                        self.save(\"cartpole-dqn.h5\")\n",
    "                        return\n",
    "                self.replay()\n",
    "\n",
    "    def test(self):\n",
    "        self.load(\"cartpole-dqn.h5\")\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = np.reshape(next_state, [1, self.state_size])\n",
    "                i += 1\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, i))\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    agent = DQNAgent()\n",
    "    agent.run()\n",
    "    #agent.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: A2C on CartPole Gym (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = gym.make('CartPole-v1')\n",
    "test_env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, actor, critic):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        action_pred = self.actor(state)\n",
    "        value_pred = self.critic(state)\n",
    "        \n",
    "        return action_pred, value_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = train_env.observation_space.shape[0]\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = test_env.action_space.n\n",
    "\n",
    "actor = MLP(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "critic = MLP(INPUT_DIM, HIDDEN_DIM, 1)\n",
    "\n",
    "policy = ActorCritic(actor, critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "\n",
    "optimizer = optim.Adam(policy.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, policy, optimizer, discount_factor):\n",
    "    \n",
    "    policy.train()\n",
    "    \n",
    "    log_prob_actions = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "        action_pred = actor(state)\n",
    "        value_pred = critic(state)\n",
    "                \n",
    "        action_prob = F.softmax(action_pred, dim = -1)\n",
    "                \n",
    "        dist = distributions.Categorical(action_prob)\n",
    "\n",
    "        action = dist.sample()\n",
    "        \n",
    "        log_prob_action = dist.log_prob(action)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        log_prob_actions.append(log_prob_action)\n",
    "        values.append(value_pred)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        episode_reward += reward\n",
    "    \n",
    "    log_prob_actions = torch.cat(log_prob_actions)\n",
    "    values = torch.cat(values).squeeze(-1)\n",
    "    \n",
    "    returns = calculate_returns(rewards, discount_factor)\n",
    "    advantages = calculate_advantages(returns, values)\n",
    "    \n",
    "    policy_loss, value_loss = update_policy(advantages, log_prob_actions, returns, values, optimizer)\n",
    "\n",
    "    return policy_loss, value_loss, episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(rewards, discount_factor, normalize = True):\n",
    "    \n",
    "    returns = []\n",
    "    R = 0\n",
    "    \n",
    "    for r in reversed(rewards):\n",
    "        R = r + R * discount_factor\n",
    "        returns.insert(0, R)\n",
    "        \n",
    "    returns = torch.tensor(returns)\n",
    "    \n",
    "    if normalize:\n",
    "        \n",
    "        returns = (returns - returns.mean()) / returns.std()\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advantages(returns, values, normalize = True):\n",
    "    \n",
    "    advantages = returns - values\n",
    "    \n",
    "    if normalize:\n",
    "        \n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "        \n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(advantages, log_prob_actions, returns, values, optimizer):\n",
    "        \n",
    "    advantages = advantages.detach()\n",
    "    returns = returns.detach()\n",
    "        \n",
    "    policy_loss = - (advantages * log_prob_actions).sum()\n",
    "    \n",
    "    value_loss = F.smooth_l1_loss(returns, values).sum()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    policy_loss.backward()\n",
    "    value_loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return policy_loss.item(), value_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy):\n",
    "    \n",
    "    policy.eval()\n",
    "    \n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            action_pred, _ = policy(state)\n",
    "\n",
    "            action_prob = F.softmax(action_pred, dim = -1)\n",
    "                \n",
    "        action = torch.argmax(action_prob, dim = -1)\n",
    "                \n",
    "        state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 500\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "N_TRIALS = 25\n",
    "REWARD_THRESHOLD = 475\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "train_rewards = []\n",
    "test_rewards = []\n",
    "\n",
    "for episode in range(1, MAX_EPISODES+1):\n",
    "    \n",
    "    policy_loss, value_loss, train_reward = train(train_env, policy, optimizer, DISCOUNT_FACTOR)\n",
    "    \n",
    "    test_reward = evaluate(test_env, policy)\n",
    "    \n",
    "    train_rewards.append(train_reward)\n",
    "    test_rewards.append(test_reward)\n",
    "    \n",
    "    mean_train_rewards = np.mean(train_rewards[-N_TRIALS:])\n",
    "    mean_test_rewards = np.mean(test_rewards[-N_TRIALS:])\n",
    "    \n",
    "    if episode % PRINT_EVERY == 0:\n",
    "    \n",
    "        print(f'| Episode: {episode:3} | Mean Train Rewards: {mean_train_rewards:5.1f} | Mean Test Rewards: {mean_test_rewards:5.1f} |')\n",
    "    \n",
    "    if mean_test_rewards >= REWARD_THRESHOLD:\n",
    "        \n",
    "        print(f'Reached reward threshold in {episode} episodes')\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(test_rewards, label='Test Reward')\n",
    "plt.plot(train_rewards, label='Train Reward')\n",
    "plt.xlabel('Episode', fontsize=20)\n",
    "plt.ylabel('Reward', fontsize=20)\n",
    "plt.hlines(REWARD_THRESHOLD, 0, len(test_rewards), color='r')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: A2C on CartPole Gym (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.losses as kls\n",
    "import tensorflow.keras.optimizers as ko\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-b', '--batch_size', type=int, default=64)\n",
    "parser.add_argument('-n', '--num_updates', type=int, default=250)\n",
    "parser.add_argument('-lr', '--learning_rate', type=float, default=7e-3)\n",
    "parser.add_argument('-r', '--render_test', action='store_true', default=False)\n",
    "parser.add_argument('-p', '--plot_results', action='store_true', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilityDistribution(tf.keras.Model):\n",
    "  def call(self, logits, **kwargs):\n",
    "    # Sample a random categorical action from the given logits.\n",
    "    return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, num_actions):\n",
    "    super().__init__('mlp_policy')\n",
    "    # Note: no tf.get_variable(), just simple Keras API!\n",
    "    self.hidden1 = kl.Dense(128, activation='relu')\n",
    "    self.hidden2 = kl.Dense(128, activation='relu')\n",
    "    self.value = kl.Dense(1, name='value')\n",
    "    # Logits are unnormalized log probabilities.\n",
    "    self.logits = kl.Dense(num_actions, name='policy_logits')\n",
    "    self.dist = ProbabilityDistribution()\n",
    "\n",
    "  def call(self, inputs, **kwargs):\n",
    "    # Inputs is a numpy array, convert to a tensor.\n",
    "    x = tf.convert_to_tensor(inputs)\n",
    "    # Separate hidden layers from the same input tensor.\n",
    "    hidden_logs = self.hidden1(x)\n",
    "    hidden_vals = self.hidden2(x)\n",
    "    return self.logits(hidden_logs), self.value(hidden_vals)\n",
    "\n",
    "  def action_value(self, obs):\n",
    "    # Executes `call()` under the hood.\n",
    "    logits, value = self.predict_on_batch(obs)\n",
    "    action = self.dist.predict_on_batch(logits)\n",
    "    # Another way to sample actions:\n",
    "    #   action = tf.random.categorical(logits, 1)\n",
    "    # Will become clearer later why we don't use it.\n",
    "    return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "  def __init__(self, model, lr=7e-3, gamma=0.99, value_c=0.5, entropy_c=1e-4):\n",
    "    # `gamma` is the discount factor; coefficients are used for the loss terms.\n",
    "    self.gamma = gamma\n",
    "    self.value_c = value_c\n",
    "    self.entropy_c = entropy_c\n",
    "\n",
    "    self.model = model\n",
    "    self.model.compile(\n",
    "      optimizer=ko.RMSprop(lr=lr),\n",
    "      # Define separate losses for policy logits and value estimate.\n",
    "      loss=[self._logits_loss, self._value_loss])\n",
    "\n",
    "  def train(self, env, batch_sz=64, updates=250):\n",
    "    # Storage helpers for a single batch of data.\n",
    "    actions = np.empty((batch_sz,), dtype=np.int32)\n",
    "    rewards, dones, values = np.empty((3, batch_sz))\n",
    "    observations = np.empty((batch_sz,) + env.observation_space.shape)\n",
    "    # Training loop: collect samples, send to optimizer, repeat updates times.\n",
    "    ep_rewards = [0.0]\n",
    "    next_obs = env.reset()\n",
    "    for update in range(updates):\n",
    "      for step in range(batch_sz):\n",
    "        observations[step] = next_obs.copy()\n",
    "        actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
    "        next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n",
    "\n",
    "        ep_rewards[-1] += rewards[step]\n",
    "        if dones[step]:\n",
    "          ep_rewards.append(0.0)\n",
    "          next_obs = env.reset()\n",
    "          logging.info(\"Episode: %03d, Reward: %03d\" % (len(ep_rewards) - 1, ep_rewards[-2]))\n",
    "\n",
    "      _, next_value = self.model.action_value(next_obs[None, :])\n",
    "      returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
    "      # A trick to input actions and advantages through same API.\n",
    "      acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
    "      # Performs a full training step on the collected batch.\n",
    "      # Note: no need to mess around with gradients, Keras API handles it.\n",
    "      losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
    "      logging.debug(\"[%d/%d] Losses: %s\" % (update + 1, updates, losses))\n",
    "\n",
    "    return ep_rewards\n",
    "\n",
    "  def test(self, env, render=False):\n",
    "    obs, done, ep_reward = env.reset(), False, 0\n",
    "    while not done:\n",
    "      action, _ = self.model.action_value(obs[None, :])\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      ep_reward += reward\n",
    "      if render:\n",
    "        env.render()\n",
    "    return ep_reward\n",
    "\n",
    "  def _returns_advantages(self, rewards, dones, values, next_value):\n",
    "    # `next_value` is the bootstrap value estimate of the future state (critic).\n",
    "    returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
    "    # Returns are calculated as discounted sum of future rewards.\n",
    "    for t in reversed(range(rewards.shape[0])):\n",
    "      returns[t] = rewards[t] + self.gamma * returns[t + 1] * (1 - dones[t])\n",
    "    returns = returns[:-1]\n",
    "    # Advantages are equal to returns - baseline (value estimates in our case).\n",
    "    advantages = returns - values\n",
    "    return returns, advantages\n",
    "\n",
    "  def _value_loss(self, returns, value):\n",
    "    # Value loss is typically MSE between value estimates and returns.\n",
    "    return self.value_c * kls.mean_squared_error(returns, value)\n",
    "\n",
    "  def _logits_loss(self, actions_and_advantages, logits):\n",
    "    # A trick to input actions and advantages through the same API.\n",
    "    actions, advantages = tf.split(actions_and_advantages, 2, axis=-1)\n",
    "    # Sparse categorical CE loss obj that supports sample_weight arg on `call()`.\n",
    "    # `from_logits` argument ensures transformation into normalized probabilities.\n",
    "    weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    # Policy loss is defined by policy gradients, weighted by advantages.\n",
    "    # Note: we only calculate the loss on the actions we've actually taken.\n",
    "    actions = tf.cast(actions, tf.int32)\n",
    "    policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
    "    # Entropy loss can be calculated as cross-entropy over itself.\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    entropy_loss = kls.categorical_crossentropy(probs, probs)\n",
    "    # We want to minimize policy and maximize entropy losses.\n",
    "    # Here signs are flipped because the optimizer minimizes.\n",
    "    return policy_loss - self.entropy_c * entropy_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  args = parser.parse_args()\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "  env = gym.make('CartPole-v0')\n",
    "  model = Model(num_actions=env.action_space.n)\n",
    "  agent = A2CAgent(model, args.learning_rate)\n",
    "\n",
    "  rewards_history = agent.train(env, args.batch_size, args.num_updates)\n",
    "  print(\"Finished training. Testing...\")\n",
    "  print(\"Total Episode Reward: %d out of 200\" % agent.test(env, args.render_test))\n",
    "\n",
    "  if args.plot_results:\n",
    "    plt.style.use('seaborn')\n",
    "    plt.plot(np.arange(0, len(rewards_history), 10), rewards_history[::10])\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: PPO on CartPole Gym (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = gym.make('CartPole-v1')\n",
    "test_env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "train_env.seed(SEED);\n",
    "test_env.seed(SEED+1);\n",
    "np.random.seed(SEED);\n",
    "torch.manual_seed(SEED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, actor, critic):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        action_pred = self.actor(state)\n",
    "        value_pred = self.critic(state)\n",
    "        \n",
    "        return action_pred, value_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = train_env.observation_space.shape[0]\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = train_env.action_space.n\n",
    "\n",
    "actor = MLP(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "critic = MLP(INPUT_DIM, HIDDEN_DIM, 1)\n",
    "\n",
    "policy = ActorCritic(actor, critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "\n",
    "optimizer = optim.Adam(policy.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, policy, optimizer, discount_factor, ppo_steps, ppo_clip):\n",
    "        \n",
    "    policy.train()\n",
    "        \n",
    "    states = []\n",
    "    actions = []\n",
    "    log_prob_actions = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "        #append state here, not after we get the next state from env.step()\n",
    "        states.append(state)\n",
    "        \n",
    "        action_pred, value_pred = policy(state)\n",
    "                \n",
    "        action_prob = F.softmax(action_pred, dim = -1)\n",
    "                \n",
    "        dist = distributions.Categorical(action_prob)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        \n",
    "        log_prob_action = dist.log_prob(action)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        actions.append(action)\n",
    "        log_prob_actions.append(log_prob_action)\n",
    "        values.append(value_pred)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        episode_reward += reward\n",
    "    \n",
    "    states = torch.cat(states)\n",
    "    actions = torch.cat(actions)    \n",
    "    log_prob_actions = torch.cat(log_prob_actions)\n",
    "    values = torch.cat(values).squeeze(-1)\n",
    "    \n",
    "    returns = calculate_returns(rewards, discount_factor)\n",
    "    advantages = calculate_advantages(returns, values)\n",
    "    \n",
    "    policy_loss, value_loss = update_policy(policy, states, actions, log_prob_actions, advantages, returns, optimizer, ppo_steps, ppo_clip)\n",
    "\n",
    "    return policy_loss, value_loss, episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(rewards, discount_factor, normalize = True):\n",
    "    \n",
    "    returns = []\n",
    "    R = 0\n",
    "    \n",
    "    for r in reversed(rewards):\n",
    "        R = r + R * discount_factor\n",
    "        returns.insert(0, R)\n",
    "        \n",
    "    returns = torch.tensor(returns)\n",
    "    \n",
    "    if normalize:\n",
    "        returns = (returns - returns.mean()) / returns.std()\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advantages(returns, values, normalize = True):\n",
    "    \n",
    "    advantages = returns - values\n",
    "    \n",
    "    if normalize:\n",
    "        \n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "        \n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(policy, states, actions, log_prob_actions, advantages, returns, optimizer, ppo_steps, ppo_clip):\n",
    "    \n",
    "    total_policy_loss = 0 \n",
    "    total_value_loss = 0\n",
    "    \n",
    "    advantages = advantages.detach()\n",
    "    log_prob_actions = log_prob_actions.detach()\n",
    "    actions = actions.detach()\n",
    "    \n",
    "    for _ in range(ppo_steps):\n",
    "                \n",
    "        #get new log prob of actions for all input states\n",
    "        action_pred, value_pred = policy(states)\n",
    "        value_pred = value_pred.squeeze(-1)\n",
    "        action_prob = F.softmax(action_pred, dim = -1)\n",
    "        dist = distributions.Categorical(action_prob)\n",
    "        \n",
    "        #new log prob using old actions\n",
    "        new_log_prob_actions = dist.log_prob(actions)\n",
    "        \n",
    "        policy_ratio = (new_log_prob_actions - log_prob_actions).exp()\n",
    "                \n",
    "        policy_loss_1 = policy_ratio * advantages\n",
    "        policy_loss_2 = torch.clamp(policy_ratio, min = 1.0 - ppo_clip, max = 1.0 + ppo_clip) * advantages\n",
    "        \n",
    "        policy_loss = - torch.min(policy_loss_1, policy_loss_2).sum()\n",
    "        \n",
    "        value_loss = F.smooth_l1_loss(returns, value_pred).sum()\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        policy_loss.backward()\n",
    "        value_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "        total_policy_loss += policy_loss.item()\n",
    "        total_value_loss += value_loss.item()\n",
    "    \n",
    "    return total_policy_loss / ppo_steps, total_value_loss / ppo_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy):\n",
    "    \n",
    "    policy.eval()\n",
    "    \n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            action_pred, _ = policy(state)\n",
    "\n",
    "            action_prob = F.softmax(action_pred, dim = -1)\n",
    "                \n",
    "        action = torch.argmax(action_prob, dim = -1)\n",
    "                \n",
    "        state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 500\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "N_TRIALS = 25\n",
    "REWARD_THRESHOLD = 475\n",
    "PRINT_EVERY = 10\n",
    "PPO_STEPS = 5\n",
    "PPO_CLIP = 0.2\n",
    "\n",
    "train_rewards = []\n",
    "test_rewards = []\n",
    "\n",
    "for episode in range(1, MAX_EPISODES+1):\n",
    "    \n",
    "    policy_loss, value_loss, train_reward = train(train_env, policy, optimizer, DISCOUNT_FACTOR, PPO_STEPS, PPO_CLIP)\n",
    "    \n",
    "    test_reward = evaluate(test_env, policy)\n",
    "    \n",
    "    train_rewards.append(train_reward)\n",
    "    test_rewards.append(test_reward)\n",
    "    \n",
    "    mean_train_rewards = np.mean(train_rewards[-N_TRIALS:])\n",
    "    mean_test_rewards = np.mean(test_rewards[-N_TRIALS:])\n",
    "    \n",
    "    if episode % PRINT_EVERY == 0:\n",
    "    \n",
    "        print(f'| Episode: {episode:3} | Mean Train Rewards: {mean_train_rewards:5.1f} | Mean Test Rewards: {mean_test_rewards:5.1f} |')\n",
    "    \n",
    "    if mean_test_rewards >= REWARD_THRESHOLD:\n",
    "        \n",
    "        print(f'Reached reward threshold in {episode} episodes')\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: PPO on CartPole Gym (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import optimizers, losses\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaes(rewards, dones, values, next_values, gamma, lamda, normalize):\n",
    "    deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
    "    deltas = np.stack(deltas)\n",
    "    gaes = copy.deepcopy(deltas)\n",
    "    for t in reversed(range(len(deltas) - 1)):\n",
    "        gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
    "\n",
    "    target = gaes + values\n",
    "    if normalize:\n",
    "        gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "    return gaes, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(Model):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.layer1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.layer2 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.layer_a1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.layer_c1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.logits = tf.keras.layers.Dense(2, activation='softmax')\n",
    "        self.value = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, state):\n",
    "        layer1 = self.layer1(state)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        \n",
    "        layer_a1 = self.layer_a1(layer2)\n",
    "        logits = self.logits(layer_a1)\n",
    "\n",
    "        layer_c1 = self.layer_c1(layer2)\n",
    "        value = self.value(layer_c1)\n",
    "\n",
    "        return logits, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.99\n",
    "        self.lamda = 0.95\n",
    "\n",
    "        self.ppo = PPO()\n",
    "        self.opt = optimizers.Adam(lr=self.lr, )\n",
    "        \n",
    "        self.rollout = 128\n",
    "        self.batch_size = 128\n",
    "        self.state_size = 4\n",
    "        self.action_size = 2\n",
    "        self.epoch = 3\n",
    "        self.ppo_eps = 0.2\n",
    "        self.normalize = True\n",
    "\n",
    "    def get_action(self, state):\n",
    "\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        policy, _ = self.ppo(state)\n",
    "        policy = np.array(policy)[0]\n",
    "        action = np.random.choice(self.action_size, p=policy)\n",
    "        return action\n",
    "\n",
    "    def update(self, state, next_state, reward, done, action):\n",
    "\n",
    "        old_policy, current_value = self.ppo(tf.convert_to_tensor(state, dtype=tf.float32))\n",
    "        _, next_value = self.ppo(tf.convert_to_tensor(next_state, dtype=tf.float32))\n",
    "        current_value, next_value = tf.squeeze(current_value), tf.squeeze(next_value)\n",
    "        current_value, next_value = np.array(current_value), np.array(next_value)\n",
    "        old_policy = np.array(old_policy)\n",
    "        \n",
    "        adv, target = get_gaes(\n",
    "            rewards=np.array(reward),\n",
    "            dones=np.array(done),\n",
    "            values=current_value,\n",
    "            next_values=next_value,\n",
    "            gamma=self.gamma,\n",
    "            lamda=self.lamda,\n",
    "            normalize=self.normalize)\n",
    "\n",
    "        for _ in range(self.epoch):\n",
    "            sample_range = np.arange(self.rollout)\n",
    "            np.random.shuffle(sample_range)\n",
    "            sample_idx = sample_range[:self.batch_size]\n",
    "            \n",
    "            batch_state = [state[i] for i in sample_idx]\n",
    "            batch_done = [done[i] for i in sample_idx]\n",
    "            batch_action = [action[i] for i in sample_idx]\n",
    "            batch_target = [target[i] for i in sample_idx]\n",
    "            batch_adv = [adv[i] for i in sample_idx]\n",
    "            batch_old_policy = [old_policy[i] for i in sample_idx]\n",
    "\n",
    "            ppo_variable = self.ppo.trainable_variables\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(ppo_variable)\n",
    "                train_policy, train_current_value = self.ppo(tf.convert_to_tensor(batch_state, dtype=tf.float32))\n",
    "                train_current_value = tf.squeeze(train_current_value)\n",
    "                train_adv = tf.convert_to_tensor(batch_adv, dtype=tf.float32)\n",
    "                train_target = tf.convert_to_tensor(batch_target, dtype=tf.float32)\n",
    "                train_action = tf.convert_to_tensor(batch_action, dtype=tf.int32)\n",
    "                train_old_policy = tf.convert_to_tensor(batch_old_policy, dtype=tf.float32)\n",
    "\n",
    "                entropy = tf.reduce_mean(-train_policy * tf.math.log(train_policy + 1e-8)) * 0.1\n",
    "                onehot_action = tf.one_hot(train_action, self.action_size)\n",
    "                selected_prob = tf.reduce_sum(train_policy * onehot_action, axis=1)\n",
    "                selected_old_prob = tf.reduce_sum(train_old_policy * onehot_action, axis=1)\n",
    "                logpi = tf.math.log(selected_prob + 1e-8)\n",
    "                logoldpi = tf.math.log(selected_old_prob + 1e-8)\n",
    "\n",
    "                ratio = tf.exp(logpi - logoldpi)\n",
    "                clipped_ratio = tf.clip_by_value(ratio, clip_value_min=1-self.ppo_eps, clip_value_max=1+self.ppo_eps)\n",
    "                minimum = tf.minimum(tf.multiply(train_adv, clipped_ratio), tf.multiply(train_adv, ratio))\n",
    "                pi_loss = -tf.reduce_mean(minimum) + entropy\n",
    "\n",
    "                value_loss = tf.reduce_mean(tf.square(train_target - train_current_value))\n",
    "\n",
    "                total_loss = pi_loss + value_loss\n",
    "\n",
    "            grads = tape.gradient(total_loss, ppo_variable)\n",
    "            self.opt.apply_gradients(zip(grads, ppo_variable))\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        env = gym.make('CartPole-v1')\n",
    "        state = env.reset()\n",
    "        episode = 0\n",
    "        score = 0\n",
    "\n",
    "        while True:\n",
    "            \n",
    "            state_list, next_state_list = [], []\n",
    "            reward_list, done_list, action_list = [], [], []\n",
    "\n",
    "            for _ in range(self.rollout):\n",
    "                \n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                score += reward\n",
    "\n",
    "                if done:\n",
    "                    if score == 500:\n",
    "                        reward = 1\n",
    "                    else:\n",
    "                        reward = -1\n",
    "                else:\n",
    "                    reward = 0\n",
    "\n",
    "                state_list.append(state)\n",
    "                next_state_list.append(next_state)\n",
    "                reward_list.append(reward)\n",
    "                done_list.append(done)\n",
    "                action_list.append(action)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    print(episode, score)\n",
    "                    state = env.reset()\n",
    "                    episode += 1\n",
    "                    score = 0\n",
    "            self.update(\n",
    "                state=state_list, next_state=next_state_list,\n",
    "                reward=reward_list, done=done_list, action=action_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    agent = Agent()\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
