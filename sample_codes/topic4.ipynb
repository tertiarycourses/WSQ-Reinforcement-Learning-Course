{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 4 Policy-Based Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient for Cartpole Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import adam\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.02\n",
    "gamma = 0.995\n",
    "episodes = 1000\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "action_space = env.action_space.n\n",
    "state_space = env.observation_space.shape[0]C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_space, 20)\n",
    "        self.fc2 = nn.Linear(20, 30)\n",
    "        self.fc3 = nn.Linear(30, action_space)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = adam.Adam(policy.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selct_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    c = Categorical(probs)\n",
    "    action = c.sample()\n",
    "\n",
    "    policy.saved_log_probs.append(c.log_prob(action))\n",
    "    action = action.item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Policy Network\n",
    "$$ R_{t} = \\sum_{k=0}^{N} \\gamma^{k}r_{t+k} \\\\\n",
    "\\Delta\\theta_t = \\alpha\\nabla_\\theta \\, \\log \\pi_\\theta (s_t,a_t)v_t  $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    rewards = []\n",
    "\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + policy.gamma * R\n",
    "        rewards.insert(0, R)\n",
    "\n",
    "    # Formalize reward\n",
    "    rewards = torch.tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean())/(rewards.std() + eps)\n",
    "\n",
    "    # get loss\n",
    "    for reward, log_prob in zip(rewards, policy.saved_log_probs):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1000):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "\n",
    "    while not done:\n",
    "        action = selct_action(obs)\n",
    "        obs, reward ,done, info = env.step(action)\n",
    "        env.render()\n",
    "        policy.rewards.append(reward)\n",
    "        score+=reward\n",
    "\n",
    "    print(f'Episode:{episode} Score:{score}')\n",
    "    if episode % 50 == 0:\n",
    "        torch.save(policy, 'policyNet.pkl')\n",
    "\n",
    "    update_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "policy = torch.load(\"policyNet.pkl\")\n",
    "\n",
    "episode = 1\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = selct_action(obs)\n",
    "        state, reward ,done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(f'Episode:{episode} Score:{score}')\n",
    "    episode +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import adam\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.02\n",
    "gamma = 0.995\n",
    "episodes = 1000\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "action_space = env.action_space.n\n",
    "state_space = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = __________\n",
    "        self.fc2 = _____________\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = ____________\n",
    "        x = _______________\n",
    "\n",
    "        return x\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = adam.Adam(policy.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selct_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    c = Categorical(probs)\n",
    "    action = c.sample()\n",
    "\n",
    "    policy.saved_log_probs.append(c.log_prob(action))\n",
    "    action = action.item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    rewards = []\n",
    "\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + policy.gamma * R\n",
    "        rewards.insert(0, R)\n",
    "\n",
    "    # Formalize reward\n",
    "    rewards = torch.tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean())/(rewards.std() + eps)\n",
    "\n",
    "    # get loss\n",
    "    for reward, log_prob in zip(rewards, policy.saved_log_probs):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1000):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "\n",
    "    while not done:\n",
    "        action = selct_action(obs)\n",
    "        obs, reward ,done, info = env.step(action)\n",
    "        env.render()\n",
    "        policy.rewards.append(reward)\n",
    "        score+=reward\n",
    "\n",
    "    print(f'Episode:{episode} Score:{score}')\n",
    "    if episode % 50 == 0:\n",
    "        torch.save(policy, 'policyNet2.pkl')\n",
    "\n",
    "    update_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "policy = torch.load(\"policyNet2.pkl\")\n",
    "\n",
    "episode = 1\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = selct_action(obs)\n",
    "        state, reward ,done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(f'Episode:{episode} Score:{score}')\n",
    "    episode +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_space, 20)\n",
    "        self.fc2 = nn.Linear(20, action_space)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = adam.Adam(policy.parameters(), lr=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
