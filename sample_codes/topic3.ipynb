{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 3 Value Based Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Value and Policy Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gridWorldGame import standard_grid, negative_grid,print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this grid gives you a reward of -0.1\n",
    "# to find a shorter path to the goal, use negative grid\n",
    "\n",
    "grid = negative_grid()\n",
    "print(\"rewards:\")\n",
    "print_values(grid.rewards, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy: state -> action\n",
    "# choose an action and update randomly \n",
    "\n",
    "policy = {}\n",
    "for s in grid.actions.keys():\n",
    "  policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "# initial policy\n",
    "print(\"initial policy:\")\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize V(s)value function\n",
    "\n",
    "V = {}\n",
    "states = grid.all_states()\n",
    "for s in states:\n",
    "  # V[s] = 0\n",
    "  if s in grid.actions:\n",
    "    V[s] = np.random.random()\n",
    "  else:\n",
    "    # terminal state\n",
    "    V[s] = 0\n",
    "\n",
    "# initial value for all states in grid\n",
    "print(V)\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Value function Iteration\n",
    "\n",
    "iteration=0\n",
    "while True:\n",
    "  iteration+=1\n",
    "  print(\"values %d: \" % iteration)\n",
    "  print_values(V, grid)\n",
    "  print(\"policy %d: \" % iteration)\n",
    "  print_policy(policy, grid)\n",
    "  \n",
    "  biggest_change = 0\n",
    "  for s in states:\n",
    "    old_v = V[s]\n",
    "\n",
    "    # V(s) only has value if it's not a terminal state\n",
    "    if s in policy:\n",
    "      new_v = float('-inf')\n",
    "      for a in ALL_POSSIBLE_ACTIONS:\n",
    "        grid.set_state(s)\n",
    "        r = grid.move(a)\n",
    "        v = r + GAMMA * V[grid.current_state()]\n",
    "        if v > new_v:\n",
    "          new_v = v\n",
    "      V[s] = new_v\n",
    "      biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "  if biggest_change < SMALL_ENOUGH:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find a policy that leads to optimal value function\n",
    "\n",
    "for s in policy.keys():\n",
    "  best_a = None\n",
    "  best_value = float('-inf')\n",
    "  # loop through all possible actions to find the best current action\n",
    "  for a in ALL_POSSIBLE_ACTIONS:\n",
    "    grid.set_state(s)\n",
    "    r = grid.move(a)\n",
    "    v = r + GAMMA * V[grid.current_state()]\n",
    "    if v > best_value:\n",
    "      best_value = v\n",
    "      best_a = a\n",
    "  policy[s] = best_a\n",
    "\n",
    "# our goal here is to verify that we get the same answer as with policy iteration\n",
    "print(\"values:\")\n",
    "print_values(V, grid)\n",
    "print(\"policy:\")\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Q-Learning in 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.95\n",
    "EPSILION = 0.9\n",
    "N_STATE = 6\n",
    "ACTIONS = ['left', 'right']\n",
    "MAX_EPISODES = 10\n",
    "FRESH_TIME = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the Q Table \n",
    "\n",
    "def build_q_table(n_state, actions):\n",
    "    q_table = pd.DataFrame(\n",
    "    np.zeros((n_state, len(actions))),\n",
    "    np.arange(n_state),\n",
    "    actions\n",
    "    )\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Action Policy\n",
    "\n",
    "def choose_action(state, q_table):\n",
    "    #epslion - greedy policy\n",
    "    state_action = q_table.loc[state,:]\n",
    "    if np.random.uniform()>EPSILION or (state_action==0).all():\n",
    "        action_name = np.random.choice(ACTIONS)\n",
    "    else:\n",
    "        action_name = state_action.idxmax()\n",
    "    return action_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Environemnt Feedback\n",
    "\n",
    "def get_env_feedback(state, action):\n",
    "    if action=='right':\n",
    "        if state == N_STATE-2:\n",
    "            next_state = 'terminal'\n",
    "            reward = 10\n",
    "        else:\n",
    "            next_state = state+1\n",
    "            reward = 1\n",
    "    else:\n",
    "        if state == 0:\n",
    "            next_state = 0\n",
    "            \n",
    "        else:\n",
    "            next_state = state-1\n",
    "        reward = -1\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update Environemnt\n",
    "\n",
    "def update_env(state,episode, step_counter):\n",
    "    env = ['-'] *(N_STATE-1)+['T']\n",
    "    if state =='terminal':\n",
    "        print(\"Episode {}, the total step is {}\".format(episode+1, step_counter))\n",
    "        final_env = ['-'] *(N_STATE-1)+['T']\n",
    "        return True, step_counter\n",
    "    else:\n",
    "        env[state]='*'\n",
    "        env = ''.join(env)\n",
    "        print(env)\n",
    "        time.sleep(FRESH_TIME)\n",
    "        return False, step_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Q Learning Algorithm\n",
    "\n",
    "def q_learning():\n",
    "    q_table = build_q_table(N_STATE, ACTIONS)\n",
    "    step_counter_times = []\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        state = 0\n",
    "        is_terminal = False\n",
    "        step_counter = 0\n",
    "        update_env(state, episode, step_counter)\n",
    "        while not is_terminal:\n",
    "            action = choose_action(state,q_table)\n",
    "            next_state, reward = get_env_feedback(state, action)\n",
    "            next_q = q_table.loc[state, action]\n",
    "            if next_state == 'terminal':\n",
    "                is_terminal = True\n",
    "                q_target = reward\n",
    "            else:\n",
    "                delta = reward + GAMMA*q_table.iloc[next_state,:].max()-q_table.loc[state, action]\n",
    "                q_table.loc[state, action] += ALPHA*delta\n",
    "            state = next_state\n",
    "            is_terminal,steps = update_env(state, episode, step_counter+1)\n",
    "            step_counter+=1\n",
    "            if is_terminal:\n",
    "                step_counter_times.append(steps)\n",
    "                \n",
    "    return q_table, step_counter_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execute the Q-learning\n",
    "\n",
    "def main():\n",
    "    q_table, step_counter_times= q_learning()\n",
    "    print(\"Q table\\n{}\\n\".format(q_table))\n",
    " \n",
    "    plt.plot(step_counter_times,'g-')\n",
    "    plt.ylabel(\"steps\")\n",
    "    plt.show()\n",
    "    print(\"The step_counter_times is {}\".format(step_counter_times))\n",
    "\n",
    "    policy = {}\n",
    "    V = np.zeros(N_STATE)\n",
    "    \n",
    "    q_table_np = q_table.to_numpy()\n",
    "\n",
    "    for S in range(N_STATE):\n",
    "        policy[S] = np.argmax(q_table_np[S,:])\n",
    "        V[S] = np.max(q_table_np[S,:])\n",
    "    print('policy :', policy)\n",
    "    print('value function: ', V)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Q-Learning in 1D (another version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "N_STATES = 6   # No of States\n",
    "N_ACTIONS = 2  # No of Actions\n",
    "EPSILON = 0.2  # greedy police\n",
    "ALPHA = 0.1     # learning rate\n",
    "GAMMA = 0.9    # discount factor\n",
    "MAX_EPISODES = 10   # maximum episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Q Tables\n",
    "\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Env Feedback\n",
    "\n",
    "def get_env_feedback(S, A):\n",
    "    if A == 1:    \n",
    "        if S == N_STATES - 2:   \n",
    "            S_ = N_STATES - 1\n",
    "            R = 10\n",
    "        else:\n",
    "            S_ = S + 1\n",
    "            R = 1\n",
    "    else:   # move left\n",
    "        R = -1\n",
    "        if S == 0:\n",
    "            S_ = S  \n",
    "        else:\n",
    "            S_ = S - 1\n",
    "    return S_, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update Env\n",
    "\n",
    "def update_env(S, episode, step_counter):\n",
    "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
    "    if S == N_STATES - 1:\n",
    "        print(' Episode {}: total_steps = {}'.format(episode+1,step_counter))\n",
    "        time.sleep(0.3)\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q-Learning\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    step_counter = 0\n",
    "    S = 0\n",
    "    is_terminated = False\n",
    "    update_env(S, episode, step_counter)\n",
    "    while not is_terminated:\n",
    "        \n",
    "        if np.random.random() < EPSILON:\n",
    "            A = np.random.randint(0, N_ACTIONS)\n",
    "        else:\n",
    "            A = np.argmax(Q[S,:])\n",
    "\n",
    "        S_, R = get_env_feedback(S, A)  \n",
    "        q_current = Q[S, A]\n",
    "        if S_ != N_STATES-1:\n",
    "            q_target = R + GAMMA*np.max(Q[S_, :])\n",
    "        else:\n",
    "            q_target = R     \n",
    "            is_terminated = True    \n",
    "\n",
    "        Q[S, A] += ALPHA * (q_target - q_current)  \n",
    "        S = S_  \n",
    "\n",
    "        update_env(S, episode, step_counter+1)\n",
    "        step_counter += 1\n",
    "\n",
    "## Final Q Table\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the final Q Table\n",
    "\n",
    "S = 0\n",
    "step_counter = 0\n",
    "is_terminated = False\n",
    "while not is_terminated:  \n",
    "    update_env(S, 0, step_counter)\n",
    "    A = np.argmax(Q[S,:])\n",
    "    S_, R = get_env_feedback(S, A)\n",
    "    step_counter += 1\n",
    "    S = S_\n",
    "    if S == N_STATES-1: \n",
    "        is_terminated = True\n",
    "print(' Total steps = ',step_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute Optimal Policy and Value Function\n",
    "\n",
    "policy = {}\n",
    "V = np.zeros(N_STATES)\n",
    "for S in range(N_STATES):\n",
    "    policy[S] = np.argmax(Q[S,:])\n",
    "    V[S] = np.max(Q[S,:])\n",
    "print('policy :', policy)\n",
    "print('value function: ', V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Q-Learnning for FrozenLake-v0 or Taxi-v3 Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "\n",
    "lr = 0.1\n",
    "gamma = 0.8\n",
    "epsilon = 0.1\n",
    "episodes = 10000\n",
    "\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "env = gym.make(\"Taxi-v3\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Q Table\n",
    "\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q-Learning\n",
    "\n",
    "for i in range(episodes):\n",
    "    print(\"Episode {}/{}\".format(i + 1, episodes))\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.random() < epsilon:\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            a = np.argmax(Q[s,:])\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        Q[s,a] += lr*(r+gamma*np.max(Q[s_,:]) - Q[s,a])\n",
    "        s = s_\n",
    "\n",
    "# Print Final Q Table\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the # of Steps and Total Rewards\n",
    "\n",
    "s = env.reset()\n",
    "done = False\n",
    "step_count = 0\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    a = np.argmax(Q[s,:])\n",
    "    s_, r, done, _ = env.step(a)\n",
    "    s = s_\n",
    "    step_count += 1\n",
    "    total_reward += r\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(\"Total steps: \",step_count)\n",
    "print(\"Total rewards: \",total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Q-Learnning with Decay Learning Rate for FrozenLake-v0 or Taxi-v3 Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "\n",
    "lr = 0.33\n",
    "lrMin = 0.001\n",
    "lrDecay = 0.9999\n",
    "gamma = 0.8\n",
    "epsilon = 1.0\n",
    "epsilonMin = 0.001\n",
    "epsilonDecay = 0.97\n",
    "episodes = 10000\n",
    "\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "env = gym.make(\"Taxi-v3\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Q Table\n",
    "\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q-Learning\n",
    "\n",
    "for i in range(episodes):\n",
    "    print(\"Episode {}/{}\".format(i + 1, episodes))\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        if np.random.random() < epsilon:\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            a = np.argmax(Q[s,:])\n",
    "        \n",
    "        s_, r, done, _ = env.step(a)\n",
    "        Q[s,a] += lr*(r+gamma*np.max(Q[s_,:]) - Q[s,a])\n",
    "        s = s_\n",
    "        \n",
    "        if lr > lrMin:\n",
    "            lr *= lrDecay\n",
    "\n",
    "        if not r==0 and epsilon > epsilonMin:\n",
    "            epsilon *= epsilonDecay\n",
    "\n",
    "### Print Final Q Table\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the # of Steps and Total Rewards\n",
    "\n",
    "s = env.reset()\n",
    "done = False\n",
    "step_count = 0\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    a = np.argmax(Q[s,:])\n",
    "    s_, r, done, _ = env.step(a)\n",
    "    s = s_\n",
    "    step_count += 1\n",
    "    total_reward += r\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(\"Total steps: \",step_count)\n",
    "print(\"Total rewards: \",total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: SARSA in 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "N_STATES = 6   # No of States\n",
    "N_ACTIONS = 2  # No of Actions \n",
    "EPSILON = 0.2  # greedy police\n",
    "ALPHA = 0.1     # learning rate\n",
    "GAMMA = 0.9    # discount factor\n",
    "MAX_EPISODES = 10   # maximum episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Q Tables\n",
    "\n",
    "Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Action Policy\n",
    "\n",
    "def choose_action(S, Q):\n",
    "    if np.random.random() < EPSILON:\n",
    "        A = np.random.randint(0, N_ACTIONS)\n",
    "    else:\n",
    "        A = np.argmax(Q[S,:])\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Environemnt Feedback\n",
    "\n",
    "def get_env_feedback(S, A):\n",
    "    if A == 1:  # move right\n",
    "        if S == N_STATES - 2:   \n",
    "            S_ = N_STATES - 1\n",
    "            R = 10\n",
    "        else:\n",
    "            S_ = S + 1\n",
    "            R = 1\n",
    "    else:   # move left\n",
    "        R = -1\n",
    "        if S == 0:\n",
    "            S_ = S  \n",
    "        else:\n",
    "            S_ = S - 1\n",
    "    return S_, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update Environemnt\n",
    "\n",
    "def update_env(S, episode, step_counter):\n",
    "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
    "    if S == N_STATES - 1:\n",
    "        print(' Episode {}: total_steps = {}'.format(episode+1,step_counter))\n",
    "        time.sleep(0.3)\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q-Learning\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    step_counter = 0\n",
    "    S = 0\n",
    "    is_terminated = False\n",
    "    update_env(S, episode, step_counter)\n",
    "    \n",
    "    A = choose_action(S, Q)\n",
    "    while not is_terminated:\n",
    "        \n",
    "        S_, R = get_env_feedback(S, A)  \n",
    "        A_ = choose_action(S_, Q)\n",
    "  \n",
    "        q_current = Q[S, A]\n",
    "        if S_ != N_STATES-1:\n",
    "            q_target = R + GAMMA*Q[S_, A_] \n",
    "        else:\n",
    "            q_target = R     \n",
    "            is_terminated = True    \n",
    "\n",
    "        Q[S, A] += ALPHA * (q_target - q_current)  \n",
    "        S = S_  \n",
    "        A = A_\n",
    "\n",
    "        update_env(S, episode, step_counter+1)\n",
    "        step_counter += 1\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the final Q Table\n",
    "\n",
    "S = 0\n",
    "step_counter = 0\n",
    "is_terminated = False\n",
    "while not is_terminated:  \n",
    "    update_env(S, 0, step_counter)\n",
    "    A = np.argmax(Q[S,:])\n",
    "    S_, R = get_env_feedback(S, A)\n",
    "    step_counter += 1\n",
    "    S = S_\n",
    "    if S == N_STATES-1: \n",
    "        is_terminated = True\n",
    "print(' Total steps = ',step_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute Optimal Policy and Value Function\n",
    "\n",
    "policy = {}\n",
    "V = np.zeros(N_STATES)\n",
    "for S in range(N_STATES):\n",
    "    policy[S] = np.argmax(Q[S,:])\n",
    "    V[S] = np.max(Q[S,:])\n",
    "print('policy :', policy)\n",
    "print('value function: ', V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: SARSA with Decay Learning Rate for FrozenLake-v0 or Taxi-v3 Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "lr = 0.33\n",
    "lrMin = 0.001\n",
    "lrDecay = 0.9999\n",
    "gamma = 1.0\n",
    "epsilon = 1.0\n",
    "epsilonMin = 0.001\n",
    "epsilonDecay = 0.97\n",
    "episodes = 2000\n",
    "\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "env = gym.make(\"Taxi-v3\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Q Table\n",
    "\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Action Policy\n",
    "\n",
    "def choose_action(s, Q):\n",
    "    if np.random.random() < epsilon:\n",
    "        a = np.random.randint(0, env.action_space.n)\n",
    "    else:\n",
    "        a = np.argmax(Q[s,:])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q-Learning\n",
    "\n",
    "for i in range(episodes):\n",
    "    print(\"Episode {}/{}\".format(i + 1, episodes))\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    a = choose_action(s, Q)\n",
    "    while not done:\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        a_ = choose_action(s_, Q)\n",
    "        Q[s,a] = Q[s,a] + lr*(r+gamma*(Q[s_,a_]) - Q[s,a])\n",
    "        s = s_\n",
    "        a = a_\n",
    "        \n",
    "        if lr > lrMin:\n",
    "            lr *= lrDecay\n",
    "\n",
    "        if not r==0 and epsilon > epsilonMin:\n",
    "            epsilon *= epsilonDecay\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the # of Steps and Total Rewards\n",
    "\n",
    "s = env.reset()\n",
    "done = False\n",
    "step_count = 0\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    a = np.argmax(Q[s,:])\n",
    "    s_, r, done, _ = env.step(a)\n",
    "    s = s_\n",
    "    step_count += 1\n",
    "    total_reward += r\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(\"Total steps: \",step_count)\n",
    "print(\"Total rewards: \",total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
